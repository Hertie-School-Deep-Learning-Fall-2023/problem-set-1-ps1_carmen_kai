{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This version is the solution to the original version provided which uses BCEWithLogitsLoss\n",
    "# We have commented out ouput_func here because BCEWithLogitsLoss combines a Sigmoid layer and the BCELoss in one single class,\n",
    "# so you do not need to apply a Sigmoid activation function in your forward pass or output layer.\n",
    "# Note: It is theoretically possibly to define the MINST classification problem as a multi-label classification problem\n",
    "# in which case using BCEWithLogitsLoss makes sense, although from a practical point of view it begs the question when \n",
    "# such a model would be useful\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class NeuralNetworkTorch(nn.Module):\n",
    "    def __init__(self, sizes, epochs=10, learning_rate=0.01, random_state=1):\n",
    "       \n",
    "        super().__init__()\n",
    "\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.random_state = random_state   \n",
    "        torch.manual_seed(self.random_state)\n",
    "\n",
    "        '''\n",
    "        TODO: Implement the forward propagation algorithm.\n",
    "        The layers should be initialized according to the sizes variable.\n",
    "        The layers should be implemented using variable size analogously to\n",
    "        the implementation in network_pytorch: sizes[i] is the shape \n",
    "        of the input that gets multiplied to the weights for the layer.\n",
    "        '''\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(len(sizes) - 1):\n",
    "            self.layers.append(nn.Linear(sizes[i], sizes[i + 1]))\n",
    "\n",
    "        self.activation_func = torch.sigmoid\n",
    "        #self.output_func = torch.softmax\n",
    "        self.loss_func = nn.BCEWithLogitsLoss()\n",
    "        self.optimizer = optim.SGD(self.parameters(), lr=learning_rate)\n",
    "\n",
    "        \n",
    "    def _forward_pass(self, x_train):\n",
    "        '''\n",
    "        TODO: The method should return the output of the network.\n",
    "        '''\n",
    "        for layer in self.layers[:-1]:\n",
    "            x_train = self.activation_func(layer(x_train))\n",
    "        return self.layers[-1](x_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _backward_pass(self, y_train, output):\n",
    "        '''\n",
    "        TODO: Implement the backpropagation algorithm responsible for updating the weights of the neural network.\n",
    "        '''\n",
    "        loss = self.loss_func(output, y_train.float())\n",
    "        loss.backward()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _update_weights(self):\n",
    "        '''\n",
    "        TODO: Update the network weights according to stochastic gradient descent.\n",
    "        '''\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "    def _flatten(self, x):\n",
    "        return x.view(x.size(0), -1)       \n",
    "\n",
    "\n",
    "    def _print_learning_progress(self, start_time, iteration, train_loader, val_loader):\n",
    "        train_accuracy = self.compute_accuracy(train_loader)\n",
    "        val_accuracy = self.compute_accuracy(val_loader)\n",
    "        print(\n",
    "            f'Epoch: {iteration + 1}, ' \\\n",
    "            f'Training Time: {time.time() - start_time:.2f}s, ' \\\n",
    "            f'Learning Rate: {self.optimizer.param_groups[0][\"lr\"]}, ' \\\n",
    "            f'Training Accuracy: {train_accuracy * 100:.2f}%, ' \\\n",
    "            f'Validation Accuracy: {val_accuracy * 100:.2f}%'\n",
    "            )\n",
    "        return train_accuracy, val_accuracy\n",
    "\n",
    "\n",
    "    def predict(self, x):\n",
    "        '''\n",
    "        TODO: Implement the prediction making of the network.\n",
    "        The method should return the index of the most likeliest output class.\n",
    "        '''\n",
    "        x = self._flatten(x)\n",
    "        output = self._forward_pass(x)\n",
    "        return torch.argmax(output, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    def fit(self, train_loader, val_loader):\n",
    "        start_time = time.time()\n",
    "        history = {'accuracy': [], 'val_accuracy': []} \n",
    "\n",
    "        for iteration in range(self.epochs): \n",
    "            for x, y in train_loader:\n",
    "                x = self._flatten(x)\n",
    "                y = nn.functional.one_hot(y, 10)\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "\n",
    "                output = self._forward_pass(x) \n",
    "                self._backward_pass(y, output)\n",
    "                self._update_weights()\n",
    "\n",
    "            train_accuracy, val_accuracy = self._print_learning_progress(start_time, iteration, train_loader, val_loader)\n",
    "            history['accuracy'].append(train_accuracy)\n",
    "            history['val_accuracy'].append(val_accuracy)\n",
    "\n",
    "        return history\n",
    "\n",
    "\n",
    "\n",
    "    def compute_accuracy(self, data_loader):\n",
    "        correct = 0\n",
    "        for x, y in data_loader:\n",
    "            pred = self.predict(x)\n",
    "            correct += torch.sum(torch.eq(pred, y))\n",
    "\n",
    "        return correct / len(data_loader.dataset)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_Lab_1_Group-A-YrkWYMVn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
